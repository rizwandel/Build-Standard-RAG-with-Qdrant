{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1f1ae2",
   "metadata": {},
   "source": [
    "## Demonstration Code\n",
    "\n",
    "This Jupyter notebook demonstrates how to use the different methods of CARAG library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06512260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from mistralai import Mistral\n",
    "import tqdm\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load environment variables\n",
    "url = os.getenv(\"url\")\n",
    "api_key = os.getenv(\"api_key\")\n",
    "mistral_api_key = str(os.getenv(\"mistral_api_key\"))\n",
    "\n",
    "if url is None:\n",
    "\tprint(\"The 'url' environment variable is not set. Please check your .env file.\")\n",
    "if api_key is None:\n",
    "\traise ValueError(\"The 'api_key' environment variable is not set. Please check your .env file.\")\n",
    "if mistral_api_key is None:\n",
    "\traise ValueError(\"The 'mistral_api_key' environment variable is not set. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8c57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carag.llm_pipeline import *\n",
    "from carag.rag_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10095a60",
   "metadata": {},
   "source": [
    "#### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7214e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = rag_pipe(url,api_key,collection_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd2a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_to_cache(\"What are the key points of the European AI Act 2024?\",\"The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime.\",0.85139245)\n",
    "#rag.search_cache(\"How much fines imposed by the European AI Act 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63566bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e468f2d4-d970-5061-945a-34bf8dc2271c',\n",
       " 'response': \"The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime.\",\n",
       " 'semantic_score': 0.9980398138892528,\n",
       " 'original_score': 0.85139245,\n",
       " 'combined_score': 0.9247161319446264}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.search_cache(\"What are the key points of the European AI Act 2024??\")\n",
    "#rag.retrieve(\"What are the key points of the European AI Act 2024?\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bb7f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id='c04ee01a-fa72-5164-a844-91d6af9b8703', version=6, score=0.85139245, payload={'response': \"The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime.\", 'meta_data': None}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='f3d08827-1658-5334-96f7-0c11a365c8d0', version=4, score=0.831648, payload={'response': 'The AI Act applies to both public and private entities inside and outside the EU, as long as their AI systems are placed on the EU market or impact individuals within the EU.', 'meta_data': 'regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of ai systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers. 12. this regulation does not apply to ai systems released under free and open - source licences, unless they are placed on the market or put into service as high - risk ai systems or as an ai system that falls under article 5 or 50. article 3 definitions for the purposes of this regulation, the following definitions apply : ( 1 ) ‘ ai system ’ means a machine - based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments ; ( 2 ) ‘ risk ’ means the combination of the probability of an occurrence of harm and the severity of that harm ; ( 3 ) ‘ provider ’ means a natural or legal person, public authority, agency or other body that develops an ai system or a general - purpose ai model or that has an ai system or a general - purpose ai model developed and places it on the market or puts the ai system into service under its own name or trademark, whether for payment or free of charge ; ( 4 ) ‘ deployer ’ means a natural or legal person, public authority, agency or other body using an ai system under its authority except where the ai system is used in the course of a personal non - professional activity ; ( 5 ) ‘ authorised representative ’ means a natural or legal person located or established in the union who has received and accepted a written mandate from a provider of an ai system or a general - purpose ai model to, respectively, perform and carry out on its behalf the obligations and procedures established by this regulation ; ( 6 ) ‘ importer ’ means a natural or legal person located or established in the union that places on the market an ai system that bears the name or trademark of a natural or legal person established in a third country ; ( 7 ) ‘ distributor ’ means a natural or legal person in the supply chain, other than the provider or the importer, that makes an ai system available on the union market ; ( 8 ) ‘ operator ’ means a provider, product manufacturer, deployer, authorised representative, importer or distributor ;'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='a7945aa7-c74e-5243-88c6-f82d4462ff2a', version=4, score=0.8316301, payload={'response': 'The AI Act aims to regulate AI systems to ensure safety, protect fundamental rights, and foster innovation while addressing risks related to health, democracy, and the environment.', 'meta_data': 'regulation ( eu ) 2024 / 1689 of the european parliament and of the council of 13 june 2024 laying down harmonised rules on artificial intelligence and amending regulations ( ec ) no 300 / 2008, ( eu ) no 167 / 2013, ( eu ) no 168 / 2013, ( eu ) 2018 / 858, ( eu ) 2018 / 1139 and ( eu ) 2019 / 2144 and directives 2014 / 90 / eu, ( eu ) 2016 / 797 and ( eu ) 2020 / 1828 ( artificial intelligence act ) ( text with eea relevance ) the european parliament and the council of the european union, having regard to the treaty on the functioning of the european union, and in particular articles 16 and 114 thereof, having regard to the proposal from the european commission, after transmission of the draft legislative act to the national parliaments, having regard to the opinion of the european economic and social committee ( 1 ), having regard to the opinion of the european central bank ( 2 ), having regard to the opinion of the committee of the regions ( 3 ), acting in accordance with the ordinary legislative procedure ( 4 ), whereas : ( 1 ) the purpose of this regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, the placing on the market, the putting into service and the use of artificial intelligence systems ( ai systems ) in the union, in accordance with union values, to promote the uptake of human centric and trustworthy artificial intelligence ( ai ) while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the charter of fundamental rights of the european union ( the ‘ charter ’ ), including democracy, the rule of law and environmental protection, to protect against the harmful effects of ai systems in the union, and to support innovation. this regulation ensures the free movement, cross - border, of ai - based goods and services, thus preventing member states from imposing restrictions on the development, marketing and use of ai systems, unless explicitly authorised by this regulation. ( 2 ) this regulation should be applied in accordance with the values of the union enshrined as in the charter, facilitating the protection of natural persons, undertakings, democracy, the rule of law and environmental protection, while boosting innovation and employment and making the union a leader in the uptake of trustworthy ai. ( 3 ) ai systems can be easily deployed in a large variety of sectors of the economy and many parts of society, including across borders, and'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='3beb2317-3cd1-5336-99a5-e493805f0cfa', version=4, score=0.8026995, payload={'response': 'High-risk AI systems include:  1. Systems used as safety components in products covered by EU laws requiring third-party conformity assessments.2. Systems under Annex III use cases, unless performing narrow procedural tasks or preparatory assessments.  3. Systems profiling individuals through automated processing of personal data.', 'meta_data': 'as high - risk pursuant to this regulation should not necessarily mean that the product whose safety component is the ai system, or the ai system itself as a product, is considered to be high - risk under the criteria established in the relevant union harmonisation legislation that applies to the product. this is, in particular, the case for regulations ( eu ) 2017 / 745 and ( eu ) 2017 / 746, where a third - party conformity assessment is provided for medium - risk and high - risk products. ( 52 ) as regards stand - alone ai systems, namely high - risk ai systems other than those that are safety components of products, or that are themselves products, it is appropriate to classify them as high - risk if, in light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre - defined areas specified in this regulation. the identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high - risk ai systems that the commission should be empowered to adopt, via delegated acts, to take into account the rapid pace of technological development, as well as the potential changes in the use of ai systems. ( 53 ) it is also important to clarify that there may be specific cases in which ai systems referred to in pre - defined areas specified in this regulation do not lead to a significant risk of harm to the legal interests protected under those areas because they do not materially influence the decision - making or do not harm those interests substantially. for the purposes of this regulation, an ai system that does not materially influence the outcome of decision - making should be understood to be an ai system that does not have an impact on the substance, and thereby the outcome, of decision - making, whether human or automated. an ai system that does not materially influence the outcome of decision - making could include situations in which one or more of the following conditions are fulfilled. the first such condition should be that the ai system is intended to perform a narrow procedural task, such as an ai system that transforms unstructured data into structured data, an ai system that classifies incoming documents into categories or an ai system that is used to detect duplicates among a large number of applications. those tasks are of such narrow and limited nature that they pose only limited risks which are not increased through the use of an ai'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='981153e7-bd62-5835-bc2a-494122a91567', version=4, score=0.79320455, payload={'response': 'Providers (developers) of high-risk AI systems bear the majority of obligations, including those placing systems on the EU market or using them in the EU, regardless of their location.', 'meta_data': 'authorities with all the necessary information on the compliance of an ai system. therefore, prior to making their ai systems available in the union, providers established in third countries should, by written mandate, appoint an authorised representative established in the union. this authorised representative plays a pivotal role in ensuring the compliance of the high - risk ai systems placed on the market or put into service in the union by those providers who are not established in the union and in serving as their contact person established in the union. ( 83 ) in light of the nature and complexity of the value chain for ai systems and in line with the new legislative framework, it is essential to ensure legal certainty and facilitate the compliance with this regulation. therefore, it is necessary to clarify the role and the specific obligations of relevant operators along that value chain, such as importers and distributors who may contribute to the development of ai systems. in certain situations those operators could act in more than one role at the same time and should therefore fulfil cumulatively all relevant obligations associated with those roles. for example, an operator could act as a distributor and an importer at the same time. ( 84 ) to ensure legal certainty, it is necessary to clarify that, under certain specific conditions, any distributor, importer, deployer or other third - party should be considered to be a provider of a high - risk ai system and therefore assume all the relevant obligations. this would be the case if that party puts its name or trademark on a high - risk ai system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are allocated otherwise. this would also be the case if that party makes a substantial modification to a high - risk ai system that has already been placed on the market or has already been put into service in a way that it remains a high - risk ai system in accordance with this regulation, or if it modifies the intended purpose of an ai system, including a general - purpose ai system, which has not been classified as high - risk and has already been placed on the market or put into service, in a way that the ai system becomes a high - risk ai system in accordance with this regulation. those provisions should apply without prejudice to more specific provisions established in certain union harmonisation legislation based on the new legislative framework, together with which this ( 38 ) directive ( eu ) 2016 / 2102 of the european parliament and of the council of 26 october 2016 on the accessibility of the websites and mobile applications of public sector bodies ( oj l 327'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='57122f7e-4ec7-5c04-a40d-2f21965d8e0e', version=4, score=0.7817549, payload={'response': '1. AI assessing migration and health risks.  2. AI analyzing asylum, visa, and residence permit applications.  3. AI recognizing individuals (excluding travel document verification). 4. AI-powered polygraphs used in migration decisions.', 'meta_data': 'of the council ( 32 ), the directive 2013 / 32 / eu of the european parliament and of the council ( 33 ), and other relevant union law. the use of ai systems in migration, asylum and border control management should, in no circumstances, be used by member states or union institutions, bodies, offices or agencies as a means to circumvent their international obligations under the un convention relating to the status of refugees done at geneva on 28 july 1951 as amended by the protocol of 31 january 1967. nor should they be used to in any way infringe on the principle of non - refoulement, or to deny safe and effective legal avenues into the territory of the union, including the right to international protection. ( 61 ) certain ai systems intended for the administration of justice and democratic processes should be classified as high - risk, considering their potentially significant impact on democracy, the rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. in particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high - risk ai systems intended to be used by a judicial authority or on its behalf to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. ai systems intended to be used by alternative dispute resolution bodies for those purposes should also be considered to be high - risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for the parties. the use of ai tools can support the decision - making power of judges or judicial independence, but should not replace it : the final decision - making must remain a human - driven activity. the classification of ai systems as high - risk should not, however, extend to ai systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks. ( 62 ) without prejudice to the rules provided for in regulation ( eu ) 2024 / 900 of the european parliament and of the council ( 34 ), and in order to address the risks of undue external interference with the right to vote enshrined in article 39 of the charter, and of adverse effects on democracy and the rule of law, ai systems intended to be used to influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda should be'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='17891434-6d44-57db-8f86-0c879354ff2c', version=4, score=0.7772572, payload={'response': '1. 6 months for prohibited AI systems.  2. 12 months for GPAI compliance.  3. 24 months for Annex III high-risk AI systems.  4. 36 months for Annex I high-risk AI systems.Codes of practice must be finalized within 9 months.', 'meta_data': 'regulation by end of 2030 and by 2 august 2030. ( 178 ) providers of high - risk ai systems are encouraged to start to comply, on a voluntary basis, with the relevant obligations of this regulation already during the transitional period. ( 179 ) this regulation should apply from 2 august 2026. however, taking into account the unacceptable risk associated with the use of ai in certain ways, the prohibitions as well as the general provisions of this regulation should already apply from 2 february 2025. while the full effect of those prohibitions follows with the establishment of the governance and enforcement of this regulation, anticipating the application of the prohibitions is important to take account of unacceptable risks and to have an effect on other procedures, such as in civil law. moreover, the infrastructure related to the governance and the conformity assessment system should be operational before 2 august 2026, therefore the provisions on notified bodies and governance structure should apply from 2 august 2025. given the rapid pace of technological advancements and adoption of general - purpose ai models, obligations for providers of general - purpose ai models should apply from 2 august 2025. codes of practice should be ready by 2 may 2025 in view of enabling providers to demonstrate compliance on time. the ai office should ensure that classification rules and procedures are up to date in light of technological developments. in addition, member states should lay down and notify to the commission the rules on penalties, including administrative fines, and ensure that they are properly and effectively implemented by the date of application of this regulation. therefore the provisions on penalties should apply from 2 august 2025. ( 180 ) the european data protection supervisor and the european data protection board were consulted in accordance with article 42 ( 1 ) and ( 2 ) of regulation ( eu ) 2018 / 1725 and delivered their joint opinion on 18 june 2021, have adopted this regulation : chapter i general provisions article 1 subject matter ` 1. the purpose of this regulation is to improve the functioning of the internal market and promote the uptake of human - centric and trustworthy artificial intelligence ( ai ), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the charter, including democracy, the rule of law and environmental protection, against the harmful effects of ai systems in the union and supporting innovation. 2. this regulation lays down : ( a ) harmonised rules for the placing on the market, the putting into service, and the use of ai systems in the union ;'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='86b28d94-a83c-52b9-b820-21ad14820127', version=4, score=0.7672869, payload={'response': '1. AI assessing an individual’s risk of becoming a crime victim.  2. AI evaluating evidence reliability.3. AI profiling individuals during investigations. 4. AI assessing likelihood of offending or re-offending (not solely based on profiling).  5. AI-powered polygraphs used in interrogations.', 'meta_data': 'single out people in a discriminatory or otherwise incorrect or unjust manner. furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such ai systems are not sufficiently transparent, explainable and documented. it is therefore appropriate to classify as high - risk, insofar as their use is permitted under relevant union and national law, a number of ai systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. in view of the nature of the activities and the risks relating thereto, those high - risk ai systems should include in particular ai systems intended to be used by or on behalf of law enforcement authorities or by union institutions, bodies, offices, or agencies in support of law enforcement authorities for assessing the risk of a natural person to become a victim of criminal offences, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course of investigation or prosecution of criminal offences, and, insofar as not prohibited under this regulation, for assessing the risk of a natural person offending or reoffending not solely on the basis of the profiling of natural persons or the assessment of personality traits and characteristics or the past criminal behaviour of natural persons or groups, for profiling in the course of detection, investigation or prosecution of criminal offences. ai systems specifically intended to be used for administrative proceedings by tax and customs authorities as well as by financial intelligence units carrying out administrative tasks analysing information pursuant to union anti - money laundering law should not be classified as high - risk ai systems used by law enforcement authorities for the purpose of prevention, detection, investigation and prosecution of criminal offences. the use of ai tools by law enforcement and other relevant authorities should not become a factor of inequality, or exclusion. the impact of the use of ai tools on the defence rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information on the functioning of those systems and the resulting difficulty in challenging their results in court, in particular by natural persons under investigation. ( 60 ) ai systems used in migration, asylum and border control management affect persons who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. the accuracy, non - discriminatory nature and transparency of the ai systems used in those'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='3479b776-1ea3-512b-a0ae-c48bb6dfcc74', version=4, score=0.7670915, payload={'response': 'Unacceptable risk—banned AI systems (e.g., social scoring, manipulative AI), High risk—strictly regulated AI systems (e.g., medical AI, recruitment AI), Limited risk—transparency obligations (e.g., chatbots, deepfakes), Minimal risk—unregulated AI systems (e.g., spam filters, video games).', 'meta_data': 'classified as high - risk ai systems with the exception of ai systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistical point of view. ( 63 ) the fact that an ai system is classified as a high - risk ai system under this regulation should not be interpreted as indicating that the use of the system is lawful under other acts of union law or under national law compatible with union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. any such use should continue to occur solely in accordance with the applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law. this regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this regulation. ( 64 ) to mitigate the risks from high - risk ai systems placed on the market or put into service and to ensure a high level of trustworthiness, certain mandatory requirements should apply to high - risk ai systems, taking into account the intended purpose and the context of use of the ai system and according to the risk - management system to be established by the provider. the measures adopted by the providers to comply with the mandatory requirements of this regulation should take into account the generally acknowledged state of the art on ai, be proportionate and effective to meet the objectives of this regulation. based on the new legislative framework, as clarified in commission notice ‘ the “ blue guide ” on the implementation of eu product rules 2022 ’, the general rule is that more than one legal act of union harmonisation legislation may be applicable to one product, since the making available or putting into service can take place only when the product complies with all applicable union harmonisation legislation. the hazards of ai systems covered by the requirements of this regulation concern different aspects than the existing union harmonisation legislation and therefore the requirements of this regulation would complement the existing body of the union harmonisation legislation. for example, machinery or medical devices products incorporating an ai system might present risks not addressed by the essential health and safety ( 32 ) regulation ( ec ) no 810 / 2009 of the european parliament and of the council of 13 july 2009 establishing a community code on visas ( visa code ) ( oj l 243, 15. 9. 2009, p. 1 ). ( 33 ) directive 2013 / 32'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='a4298e62-634c-5dba-89ab-a05900355166', version=4, score=0.7668414, payload={'response': 'Deployers have fewer obligations than providers but must ensure compliance when deploying high-risk AI systems in a professional capacity. This applies to deployers in the EU and third-country users whose AI system outputs are used in the EU.', 'meta_data': '2. deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support. 3. the obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under union or national law and to the deployer ’ s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. 4. without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high - risk ai system. 5. deployers shall monitor the operation of the high - risk ai system on the basis of the instructions for use and, where relevant, inform providers in accordance with article 72. where deployers have reason to consider that the use of the high - risk ai system in accordance with the instructions may result in that ai system presenting a risk within the meaning of article 79 ( 1 ), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system. where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. if the deployer is not able to reach the provider, article 73 shall apply mutatis mutandis. this obligation shall not cover sensitive operational data of deployers of ai systems which are law enforcement authorities. for deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under union financial services law, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to the relevant financial service law. 6. deployers of high - risk ai systems shall keep the logs automatically generated by that high - risk ai system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high - risk ai system, of at least six months, unless provided otherwise in applicable union or national law, in particular in union law on the protection of personal data. deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under union financial services law shall maintain the logs as part of the documentation kept pursuant to the relevant union financial service law. 7. before putting into service or using a high - risk ai'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='0b0f8d50-9def-5efc-a0bb-d3870619ce1a', version=4, score=0.7636027, payload={'response': 'The AI Office can evaluate GPAI models to: 1. Assess compliance when information is insufficient.  2. Investigate systemic risks, especially after reports from independent experts.', 'meta_data': '( d ) and ( e ) thereof, the commission is empowered to adopt delegated acts in accordance with article 97 to detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation. 6. the commission is empowered to adopt delegated acts in accordance with article 97 ( 2 ) to amend annexes xi and xii in light of evolving technological developments. 7. any information or documentation obtained pursuant to this article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in article 78. article 54 authorised representatives of providers of general - purpose ai models 1. prior to placing a general - purpose ai model on the union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the union. 2. the provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider. 3. the authorised representative shall perform the tasks specified in the mandate received from the provider. it shall provide a copy of the mandate to the ai office upon request, in one of the official languages of the institutions of the union. for the purposes of this regulation, the mandate shall empower the authorised representative to carry out the following tasks : ( a ) verify that the technical documentation specified in annex xi has been drawn up and all obligations referred to in article 53 and, where applicable, article 55 have been fulfilled by the provider ; ( b ) keep a copy of the technical documentation specified in annex xi at the disposal of the ai office and national competent authorities, for a period of 10 years after the general - purpose ai model has been placed on the market, and the contact details of the provider that appointed the authorised representative ; ( c ) provide the ai office, upon a reasoned request, with all the information and documentation, including that referred to in point ( b ), necessary to demonstrate compliance with the obligations in this chapter ; ( d ) cooperate with the ai office and competent authorities, upon a reasoned request, in any action they take in relation to the general - purpose ai model, including when the model is integrated into ai systems placed on the market or put into service in the union. 4. the mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the ai office or the competent authorities, on all issues related to ensuring compliance with this regulation.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='856f0979-c546-5c0c-b929-a3ce0d57c3f5', version=4, score=0.7604859, payload={'response': '1. Biometric categorization systems inferring sensitive attributes.  2. AI systems managing critical infrastructure like water and electricity supply.3. Systems determining access to education or vocational training.4. Recruitment and worker management systems.5. AI systems assessing eligibility for public services or creditworthiness.  6. Law enforcement systems evaluating evidence reliability or profiling individuals.  7. Migration and border control systems assessing asylum applications. 8. Systems influencing elections or voting behavior.', 'meta_data': 'annex iii high - risk ai systems referred to in article 6 ( 2 ) high - risk ai systems pursuant to article 6 ( 2 ) are the ai systems listed in any of the following areas : 1. biometrics, in so far as their use is permitted under relevant union or national law : ( a ) remote biometric identification systems. this shall not include ai systems intended to be used for biometric verification the sole purpose of which is to confirm that a specific natural person is the person he or she claims to be ; ( b ) ai systems intended to be used for biometric categorisation, according to sensitive or protected attributes or characteristics based on the inference of those attributes or characteristics ; ( c ) ai systems intended to be used for emotion recognition. 2. critical infrastructure : ai systems intended to be used as safety components in the management and operation of critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity. 3. education and vocational training : ( a ) ai systems intended to be used to determine access or admission or to assign natural persons to educational and vocational training institutions at all levels ; ( b ) ai systems intended to be used to evaluate learning outcomes, including when those outcomes are used to steer the learning process of natural persons in educational and vocational training institutions at all levels ; ( c ) ai systems intended to be used for the purpose of assessing the appropriate level of education that an individual will receive or will be able to access, in the context of or within educational and vocational training institutions at all levels ; ( d ) ai systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in the context of or within educational and vocational training institutions at all levels. 4. employment, workers ’ management and access to self - employment : ( a ) ai systems intended to be used for the recruitment or selection of natural persons, in particular to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates ; ( b ) ai systems intended to be used to make decisions affecting terms of work - related relationships, the promotion or termination of work - related contractual relationships, to allocate tasks based on individual behaviour or personal traits or characteristics or to monitor and evaluate the performance and behaviour of persons in such relationships. 5. access to and enjoyment of essential private services and essential public services and benefits : ( a ) ai systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for essential public assistance benefits and services, including'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='749a84ca-6d49-5534-921f-f9757d32717d', version=4, score=0.7587357, payload={'response': '1. Implement a risk management system throughout the AI lifecycle.2. Ensure training datasets are representative and error-free.3. Maintain technical documentation demonstrating compliance.4. Enable automatic record-keeping for risk identification.5. Provide instructions for compliance to deployers.6. Design systems with human oversight mechanisms', 'meta_data': 'section 3 obligations of providers and deployers of high - risk ai systems and other parties article 16 obligations of providers of high - risk ai systems providers of high - risk ai systems shall : ( a ) ensure that their high - risk ai systems are compliant with the requirements set out in section 2 ; ( b ) indicate on the high - risk ai system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted ; ( c ) have a quality management system in place which complies with article 17 ; ( d ) keep the documentation referred to in article 18 ; ( e ) when under their control, keep the logs automatically generated by their high - risk ai systems as referred to in article 19 ; ( f ) ensure that the high - risk ai system undergoes the relevant conformity assessment procedure as referred to in article 43, prior to its being placed on the market or put into service ; ( g ) draw up an eu declaration of conformity in accordance with article 47 ; ( h ) affix the ce marking to the high - risk ai system or, where that is not possible, on its packaging or its accompanying documentation, to indicate conformity with this regulation, in accordance with article 48 ; ( i ) comply with the registration obligations referred to in article 49 ( 1 ) ; ( j ) take the necessary corrective actions and provide information as required in article 20 ; ( k ) upon a reasoned request of a national competent authority, demonstrate the conformity of the high - risk ai system with the requirements set out in section 2 ; ( l ) ensure that the high - risk ai system complies with accessibility requirements in accordance with directives ( eu ) 2016 / 2102 and ( eu ) 2019 / 882. article 17 quality management system 1. providers of high - risk ai systems shall put a quality management system in place that ensures compliance with this regulation. that system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects : ( a ) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high - risk ai system ; ( b ) techniques, procedures and systematic actions to be used for the design, design control and design verification of the high - risk ai system ; ( c ) techniques, procedures and systematic actions to be used for the development, quality control and'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='a17f3e8f-d21b-5aaf-b398-588c51bb07df', version=4, score=0.74525964, payload={'response': 'A GPAI model is systemic if its training compute exceeds \\\\\\\\( 10^{25} \\\\\\\\) FLOPS. Providers must notify the European Commission within two weeks of exceeding this threshold.', 'meta_data': 'of general - purpose ai models as general - purpose ai model with systemic risks. since systemic risks result from particularly high capabilities, a general - purpose ai model should be considered to present systemic risks if it has high - impact capabilities, evaluated on the basis of appropriate technical tools and methodologies, or significant impact on the internal market due to its reach. high - impact capabilities in general - purpose ai models means capabilities that match or exceed the capabilities recorded in the most advanced general - purpose ai models. the full range of capabilities in a model could be better understood after its placing on the market or when deployers interact with the model. according to the state of the art at the time of entry into force of this regulation, the cumulative amount of computation used for the training of the general - purpose ai model measured in floating point operations is one of the relevant approximations for model capabilities. the cumulative amount of computation used for training includes the computation used across the activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre - training, synthetic data generation and fine - tuning. therefore, an initial threshold of floating point operations should be set, which, if met by a general - purpose ai model, leads to a presumption that the model is a general - purpose ai model with systemic risks. this threshold should be adjusted over time to reflect technological and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be supplemented with benchmarks and indicators for model capability. to inform this, the ai office should engage with the scientific community, industry, civil society and other experts. thresholds, as well as tools and benchmarks for the assessment of high - impact capabilities, should be strong predictors of generality, its capabilities and associated systemic risk of general - purpose ai models, and could take into account the way the model will be placed on the market or the number of users it may affect. to complement this system, there should be a possibility for the commission to take individual decisions designating a general - purpose ai model as a general - purpose ai model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold. that decision should be taken on the basis of an overall assessment of the criteria for the designation of a general - purpose ai model with systemic risk set out in an annex to this regulation, such as quality or size of the training data set, number of business and end users, its input and output modalities, its level of'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='6d19d079-a999-52da-b066-69c77d098627', version=4, score=0.7416184, payload={'response': '1. AI systems using manipulative techniques to distort behavior and impair decision-making.  2. Social scoring systems evaluating individuals based on personal traits.3. Systems assessing criminal risk solely based on profiling.4. Untargeted facial recognition databases.5. Emotion inference in workplaces or schools (except for medical or safety reasons).  6. Real-time remote biometric identification in public spaces (with exceptions for law enforcement).', 'meta_data': 'article 4 ai literacy providers and deployers of ai systems shall take measures to ensure, to their best extent, a sufficient level of ai literacy of their staff and other persons dealing with the operation and use of ai systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the ai systems are to be used in, and considering the persons or groups of persons on whom the ai systems are to be used. chapter ii prohibited ai practices article 5 prohibited ai practices 1. the following ai practices shall be prohibited : ( a ) the placing on the market, the putting into service or the use of an ai system that deploys subliminal techniques beyond a person ’ s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm ; ( b ) the placing on the market, the putting into service or the use of an ai system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm ; ( c ) the placing on the market, the putting into service or the use of ai systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following : ( i ) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected ; ( ii ) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity ; ( d ) the placing on the market, the putting into service for this specific purpose, or the use of an ai system for making risk assessments of natural persons in order to assess or predict the risk of a'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='c8581c61-1349-5c54-b587-382b73dcc9da', version=4, score=0.737363, payload={'response': 'High-risk AI systems must be designed to allow deployers to implement human oversight, ensuring that decisions can be reviewed and overridden if necessary.', 'meta_data': '12. article 14 human oversight 1. high - risk ai systems shall be designed and developed in such a way, including with appropriate human - machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use. 2. human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high - risk ai system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this section. 3. the oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high - risk ai system, and shall be ensured through either one or both of the following types of measures : ( a ) measures identified and built, when technically feasible, into the high - risk ai system by the provider before it is placed on the market or put into service ; ( b ) measures identified by the provider before placing the high - risk ai system on the market or putting it into service and that are appropriate to be implemented by the deployer. 4. for the purpose of implementing paragraphs 1, 2 and 3, the high - risk ai system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate : ( a ) to properly understand the relevant capacities and limitations of the high - risk ai system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance ; ( b ) to remain aware of the possible tendency of automatically relying or over - relying on the output produced by a high - risk ai system ( automation bias ), in particular for high - risk ai systems used to provide information or recommendations for decisions to be taken by natural persons ; ( c ) to correctly interpret the high - risk ai system ’ s output, taking into account, for example, the interpretation tools and methods available ;'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='b029fb58-c29b-5c71-863e-c36bbdaa9c40', version=4, score=0.7353422, payload={'response': 'AI systems that interact with voters to influence voting behavior are high-risk, while AI tools that optimize campaign logistics without direct voter engagement are not classified as high-risk.', 'meta_data': 'applying the law to a concrete set of facts. ai systems intended to be used by alternative dispute resolution bodies for those purposes should also be considered to be high - risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for the parties. the use of ai tools can support the decision - making power of judges or judicial independence, but should not replace it : the final decision - making must remain a human - driven activity. the classification of ai systems as high - risk should not, however, extend to ai systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks. ( 62 ) without prejudice to the rules provided for in regulation ( eu ) 2024 / 900 of the european parliament and of the council ( 34 ), and in order to address the risks of undue external interference with the right to vote enshrined in article 39 of the charter, and of adverse effects on democracy and the rule of law, ai systems intended to be used to influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda should be classified as high - risk ai systems with the exception of ai systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistical point of view. ( 63 ) the fact that an ai system is classified as a high - risk ai system under this regulation should not be interpreted as indicating that the use of the system is lawful under other acts of union law or under national law compatible with union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. any such use should continue to occur solely in accordance with the applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law. this regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this regulation. ( 64 ) to mitigate the risks from high - risk ai systems placed on the market or put into service and to ensure a high level of trustworthiness, certain mandatory requirements should apply to high - risk ai systems, taking into account the intended purpose and the context of use of the ai system'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='409a2f2f-6cee-53bd-8882-4155f9c92ad0', version=4, score=0.7340894, payload={'response': 'Providers must establish a risk management system throughout the AI system’s lifecycle, addressing risks related to health, safety, and fundamental rights.', 'meta_data': '2. where a product contains an ai system, to which the requirements of this regulation as well as requirements of the union harmonisation legislation listed in section a of annex i apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable union harmonisation legislation. in ensuring the compliance of high - risk ai systems referred to in paragraph 1 with the requirements set out in this section, and in order to ensure consistency, avoid duplication and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into documentation and procedures that already exist and are required under the union harmonisation legislation listed in section a of annex i. article 9 risk management system 1. a risk management system shall be established, implemented, documented and maintained in relation to high - risk ai systems. 2. the risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high - risk ai system, requiring regular systematic review and updating. it shall comprise the following steps : ( a ) the identification and analysis of the known and the reasonably foreseeable risks that the high - risk ai system can pose to health, safety or fundamental rights when the high - risk ai system is used in accordance with its intended purpose ; ( b ) the estimation and evaluation of the risks that may emerge when the high - risk ai system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse ; ( c ) the evaluation of other risks possibly arising, based on the analysis of data gathered from the post - market monitoring system referred to in article 72 ; ( d ) the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point ( a ). 3. the risks referred to in this article shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high - risk ai system, or the provision of adequate technical information. 4. the risk management measures referred to in paragraph 2, point ( d ), shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this section, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements. 5. the risk management measures referred to in paragraph 2, point ( d ), shall be such that the relevant residual risk associated with each hazard,'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='30c8b5a3-4863-5906-aa42-8f0fc9056d7d', version=4, score=0.7333877, payload={'response': 'Developers and deployers must ensure that end-users are aware they are interacting with AI, such as chatbots and deepfakes.', 'meta_data': 'specific ai system for the purposes of this regulation. that initial provider shall closely cooperate with new providers and shall make available the necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the obligations set out in this regulation, in particular regarding the compliance with the conformity assessment of high - risk ai systems. this paragraph shall not apply in cases where the initial provider has clearly specified that its ai system is not to be changed into a high - risk ai system and therefore does not fall under the obligation to hand over the documentation. 3. in the case of high - risk ai systems that are safety components of products covered by the union harmonisation legislation listed in section a of annex i, the product manufacturer shall be considered to be the provider of the high - risk ai system, and shall be subject to the obligations under article 16 under either of the following circumstances : ( a ) the high - risk ai system is placed on the market together with the product under the name or trademark of the product manufacturer ; ( b ) the high - risk ai system is put into service under the name or trademark of the product manufacturer after the product has been placed on the market. 4. the provider of a high - risk ai system and the third party that supplies an ai system, tools, services, components, or processes that are used or integrated in a high - risk ai system shall, by written agreement, specify the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high - risk ai system to fully comply with the obligations set out in this regulation. this paragraph shall not apply to third parties making accessible to the public tools, services, processes, or components, other than general - purpose ai models, under a free and open - source licence. the ai office may develop and recommend voluntary model terms for contracts between providers of high - risk ai systems and third parties that supply tools, services, components or processes that are used for or integrated into high - risk ai systems. when developing those voluntary model terms, the ai office shall take into account possible contractual requirements applicable in specific sectors or business cases. the voluntary model terms shall be published and be available free of charge in an easily usable electronic format. 5. paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets in accordance with union and national law. article 26 obligations of deployers of high - risk ai systems'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='88dc63e0-fd0a-580c-99fa-5ec0ae9cd0cc', version=4, score=0.72543985, payload={'response': '1. AI filtering and analyzing job applications.2. AI evaluating candidate suitability. 3. AI allocating tasks based on performance metrics.  4. AI making promotion and termination decisions.', 'meta_data': 'regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of ai systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers. 12. this regulation does not apply to ai systems released under free and open - source licences, unless they are placed on the market or put into service as high - risk ai systems or as an ai system that falls under article 5 or 50. article 3 definitions for the purposes of this regulation, the following definitions apply : ( 1 ) ‘ ai system ’ means a machine - based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments ; ( 2 ) ‘ risk ’ means the combination of the probability of an occurrence of harm and the severity of that harm ; ( 3 ) ‘ provider ’ means a natural or legal person, public authority, agency or other body that develops an ai system or a general - purpose ai model or that has an ai system or a general - purpose ai model developed and places it on the market or puts the ai system into service under its own name or trademark, whether for payment or free of charge ; ( 4 ) ‘ deployer ’ means a natural or legal person, public authority, agency or other body using an ai system under its authority except where the ai system is used in the course of a personal non - professional activity ; ( 5 ) ‘ authorised representative ’ means a natural or legal person located or established in the union who has received and accepted a written mandate from a provider of an ai system or a general - purpose ai model to, respectively, perform and carry out on its behalf the obligations and procedures established by this regulation ; ( 6 ) ‘ importer ’ means a natural or legal person located or established in the union that places on the market an ai system that bears the name or trademark of a natural or legal person established in a third country ; ( 7 ) ‘ distributor ’ means a natural or legal person in the supply chain, other than the provider or the importer, that makes an ai system available on the union market ; ( 8 ) ‘ operator ’ means a provider, product manufacturer, deployer, authorised representative, importer or distributor ;'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    }
   ],
   "source": [
    "s= rag.invoke(url, api_key, \"What are the key points of the European AI Act 2024?\", 'test') # invoke the RAG pipeline\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a93d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"test\"\n",
    "gg = GroundGeneration(url, api_key, mistral_api_key,collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755d56cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_results': [{'id': 'result_id_1', 'answer': 'The European AI Act 2024 is a comprehensive regulatory framework aimed at ensuring the safe and ethical development, deployment, and use of artificial intelligence (AI) within the European Union. Key points of the Act include the classification of AI systems into different risk categories, with those deemed high-risk subject to stricter regulations. The Act also establishes a governance structure to oversee compliance and enforcement, including the creation of a European AI Board. Additionally, the Act emphasizes the importance of transparency, accountability, and human oversight in AI systems. It mandates that AI systems be designed and developed in a way that respects fundamental rights and European values, such as privacy, non-discrimination, and fairness. The Act also includes provisions for liability and redress mechanisms, ensuring that individuals harmed by AI systems have recourse. Furthermore, the Act promotes innovation and investment in AI while fostering a competitive and trustworthy AI ecosystem within the EU.', 'reason': 'This result provides a comprehensive overview of the European AI Act 2024, covering key points such as risk classification, governance, transparency, and liability, making it highly relevant to the query.'}, {'id': 'result_id_2', 'answer': 'The European AI Act 2024 introduces a risk-based approach to regulating AI, with AI systems categorized as unacceptable risk, high risk, or limited/minimal risk. High-risk AI systems, such as those used in critical infrastructure or for biometric identification, are subject to strict obligations before they can be put on the market. These obligations include risk management systems, data and data governance requirements, technical documentation, record-keeping, transparency, and human oversight. The Act also requires providers of high-risk AI systems to conduct conformity assessments and undergo mandatory third-party testing. Additionally, the Act establishes a regulatory sandboxing scheme to facilitate the development and testing of innovative AI systems in a controlled environment. The Act also addresses the use of remote biometric identification systems in publicly accessible spaces, requiring prior authorization and consultation with relevant authorities.', 'reason': \"This result delves into the risk-based approach of the European AI Act 2024, providing detailed information on the categorization of AI systems and the obligations for high-risk systems, which is crucial for understanding the Act's regulatory framework.\"}, {'id': 'result_id_3', 'answer': 'The European AI Act 2024 places a strong emphasis on the protection of fundamental rights and the establishment of clear liability rules for AI systems. The Act requires that AI systems be designed and developed in a manner that prevents discriminatory outcomes and ensures accessibility for persons with disabilities. It also mandates that users be informed when they are interacting with an AI system, unless this is obvious from the circumstances. The Act introduces ex-ante requirements for high-risk AI systems, including the creation of a European database to register stand-alone high-risk AI systems before they are placed on the market or put into service. Furthermore, the Act establishes the European Artificial Intelligence Board, which will facilitate the implementation of the Act and provide advice and expertise to the Commission and Member States. The Act also includes provisions for international cooperation, encouraging the development of global standards for AI.', 'reason': \"This result highlights the Act's focus on fundamental rights, liability rules, and the establishment of governance structures, providing a well-rounded view of the Act's provisions and their implications for AI development and use.\"}]}\n",
      "--------------------\n",
      "{'top_results': [{'id': 'result_id_1', 'answer': 'The European AI Act, set to be enforced in 2024, imposes significant fines for non-compliance. The fines can range up to €30 million or 6% of the global annual turnover, whichever is higher. These penalties are designed to ensure that companies adhere to the stringent regulations aimed at promoting ethical and responsible AI development and deployment. The Act covers a broad spectrum of AI applications, with particular emphasis on high-risk AI systems. Companies must conduct risk assessments, implement mitigation measures, and maintain transparency in their AI operations to avoid these hefty fines.', 'reason': 'This result directly addresses the query about the fines imposed by the European AI Act 2024, providing specific details on the penalty amounts and the context in which they are applied.'}, {'id': 'result_id_2', 'answer': 'Under the European AI Act 2024, organizations found to be non-compliant with the regulations will face substantial fines. The maximum fine is set at €30 million or 6% of the global annual turnover, depending on which is greater. This regulatory framework is part of a broader effort to ensure that AI technologies are developed and used in a manner that respects fundamental rights and values. The Act requires companies to implement robust risk management systems and maintain high standards of transparency and accountability in their AI practices.', 'reason': 'This result is highly relevant as it specifies the maximum fines and provides additional context about the regulatory framework and the requirements for companies to comply with the Act.'}, {'id': 'result_id_3', 'answer': 'The European AI Act, scheduled to take effect in 2024, introduces stringent penalties for non-compliance with its regulations. Fines can reach up to €30 million or 6% of the global annual turnover, whichever is higher. These penalties are part of a comprehensive approach to regulate AI, ensuring that it is used ethically and responsibly. The Act mandates that companies conduct thorough risk assessments, implement appropriate mitigation measures, and adhere to transparency and accountability standards. Failure to comply with these requirements will result in the imposition of the aforementioned fines.', 'reason': 'This result is selected because it clearly states the fines and provides a detailed overview of the regulatory requirements and the consequences of non-compliance, aligning well with the query.'}]}\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What are the key points of the European AI Act 2024?\"\n",
    "query2 = \"How much fines imposed by the European AI Act 2024?\"\n",
    "result_1 = gg.grounded_generation_from_llm(query1)\n",
    "result_2 = gg.grounded_generation_from_llm(query2)\n",
    "print(result_1)\n",
    "print(\"--------------------\")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4d92d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e468f2d4-d970-5061-945a-34bf8dc2271c',\n",
       " 'response': \"The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime.\",\n",
       " 'cache_hit': True,\n",
       " 'semantic_score': 1.0000000018269146,\n",
       " 'original_score': 0.85139245,\n",
       " 'elapsed_time': 0.3745110034942627}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.retrieve(\"What are the key points of the European AI Act 2024?\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83636fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='c04ee01a-fa72-5164-a844-91d6af9b8703', version=6, score=0.85139245, payload={'response': \"The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime.\", 'meta_data': None}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='f3d08827-1658-5334-96f7-0c11a365c8d0', version=4, score=0.831648, payload={'response': 'The AI Act applies to both public and private entities inside and outside the EU, as long as their AI systems are placed on the EU market or impact individuals within the EU.', 'meta_data': 'regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of ai systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers. 12. this regulation does not apply to ai systems released under free and open - source licences, unless they are placed on the market or put into service as high - risk ai systems or as an ai system that falls under article 5 or 50. article 3 definitions for the purposes of this regulation, the following definitions apply : ( 1 ) ‘ ai system ’ means a machine - based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments ; ( 2 ) ‘ risk ’ means the combination of the probability of an occurrence of harm and the severity of that harm ; ( 3 ) ‘ provider ’ means a natural or legal person, public authority, agency or other body that develops an ai system or a general - purpose ai model or that has an ai system or a general - purpose ai model developed and places it on the market or puts the ai system into service under its own name or trademark, whether for payment or free of charge ; ( 4 ) ‘ deployer ’ means a natural or legal person, public authority, agency or other body using an ai system under its authority except where the ai system is used in the course of a personal non - professional activity ; ( 5 ) ‘ authorised representative ’ means a natural or legal person located or established in the union who has received and accepted a written mandate from a provider of an ai system or a general - purpose ai model to, respectively, perform and carry out on its behalf the obligations and procedures established by this regulation ; ( 6 ) ‘ importer ’ means a natural or legal person located or established in the union that places on the market an ai system that bears the name or trademark of a natural or legal person established in a third country ; ( 7 ) ‘ distributor ’ means a natural or legal person in the supply chain, other than the provider or the importer, that makes an ai system available on the union market ; ( 8 ) ‘ operator ’ means a provider, product manufacturer, deployer, authorised representative, importer or distributor ;'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='a7945aa7-c74e-5243-88c6-f82d4462ff2a', version=4, score=0.8316301, payload={'response': 'The AI Act aims to regulate AI systems to ensure safety, protect fundamental rights, and foster innovation while addressing risks related to health, democracy, and the environment.', 'meta_data': 'regulation ( eu ) 2024 / 1689 of the european parliament and of the council of 13 june 2024 laying down harmonised rules on artificial intelligence and amending regulations ( ec ) no 300 / 2008, ( eu ) no 167 / 2013, ( eu ) no 168 / 2013, ( eu ) 2018 / 858, ( eu ) 2018 / 1139 and ( eu ) 2019 / 2144 and directives 2014 / 90 / eu, ( eu ) 2016 / 797 and ( eu ) 2020 / 1828 ( artificial intelligence act ) ( text with eea relevance ) the european parliament and the council of the european union, having regard to the treaty on the functioning of the european union, and in particular articles 16 and 114 thereof, having regard to the proposal from the european commission, after transmission of the draft legislative act to the national parliaments, having regard to the opinion of the european economic and social committee ( 1 ), having regard to the opinion of the european central bank ( 2 ), having regard to the opinion of the committee of the regions ( 3 ), acting in accordance with the ordinary legislative procedure ( 4 ), whereas : ( 1 ) the purpose of this regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, the placing on the market, the putting into service and the use of artificial intelligence systems ( ai systems ) in the union, in accordance with union values, to promote the uptake of human centric and trustworthy artificial intelligence ( ai ) while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the charter of fundamental rights of the european union ( the ‘ charter ’ ), including democracy, the rule of law and environmental protection, to protect against the harmful effects of ai systems in the union, and to support innovation. this regulation ensures the free movement, cross - border, of ai - based goods and services, thus preventing member states from imposing restrictions on the development, marketing and use of ai systems, unless explicitly authorised by this regulation. ( 2 ) this regulation should be applied in accordance with the values of the union enshrined as in the charter, facilitating the protection of natural persons, undertakings, democracy, the rule of law and environmental protection, while boosting innovation and employment and making the union a leader in the uptake of trustworthy ai. ( 3 ) ai systems can be easily deployed in a large variety of sectors of the economy and many parts of society, including across borders, and'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='3beb2317-3cd1-5336-99a5-e493805f0cfa', version=4, score=0.8026995, payload={'response': 'High-risk AI systems include:  1. Systems used as safety components in products covered by EU laws requiring third-party conformity assessments.2. Systems under Annex III use cases, unless performing narrow procedural tasks or preparatory assessments.  3. Systems profiling individuals through automated processing of personal data.', 'meta_data': 'as high - risk pursuant to this regulation should not necessarily mean that the product whose safety component is the ai system, or the ai system itself as a product, is considered to be high - risk under the criteria established in the relevant union harmonisation legislation that applies to the product. this is, in particular, the case for regulations ( eu ) 2017 / 745 and ( eu ) 2017 / 746, where a third - party conformity assessment is provided for medium - risk and high - risk products. ( 52 ) as regards stand - alone ai systems, namely high - risk ai systems other than those that are safety components of products, or that are themselves products, it is appropriate to classify them as high - risk if, in light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre - defined areas specified in this regulation. the identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high - risk ai systems that the commission should be empowered to adopt, via delegated acts, to take into account the rapid pace of technological development, as well as the potential changes in the use of ai systems. ( 53 ) it is also important to clarify that there may be specific cases in which ai systems referred to in pre - defined areas specified in this regulation do not lead to a significant risk of harm to the legal interests protected under those areas because they do not materially influence the decision - making or do not harm those interests substantially. for the purposes of this regulation, an ai system that does not materially influence the outcome of decision - making should be understood to be an ai system that does not have an impact on the substance, and thereby the outcome, of decision - making, whether human or automated. an ai system that does not materially influence the outcome of decision - making could include situations in which one or more of the following conditions are fulfilled. the first such condition should be that the ai system is intended to perform a narrow procedural task, such as an ai system that transforms unstructured data into structured data, an ai system that classifies incoming documents into categories or an ai system that is used to detect duplicates among a large number of applications. those tasks are of such narrow and limited nature that they pose only limited risks which are not increased through the use of an ai'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='981153e7-bd62-5835-bc2a-494122a91567', version=4, score=0.79320455, payload={'response': 'Providers (developers) of high-risk AI systems bear the majority of obligations, including those placing systems on the EU market or using them in the EU, regardless of their location.', 'meta_data': 'authorities with all the necessary information on the compliance of an ai system. therefore, prior to making their ai systems available in the union, providers established in third countries should, by written mandate, appoint an authorised representative established in the union. this authorised representative plays a pivotal role in ensuring the compliance of the high - risk ai systems placed on the market or put into service in the union by those providers who are not established in the union and in serving as their contact person established in the union. ( 83 ) in light of the nature and complexity of the value chain for ai systems and in line with the new legislative framework, it is essential to ensure legal certainty and facilitate the compliance with this regulation. therefore, it is necessary to clarify the role and the specific obligations of relevant operators along that value chain, such as importers and distributors who may contribute to the development of ai systems. in certain situations those operators could act in more than one role at the same time and should therefore fulfil cumulatively all relevant obligations associated with those roles. for example, an operator could act as a distributor and an importer at the same time. ( 84 ) to ensure legal certainty, it is necessary to clarify that, under certain specific conditions, any distributor, importer, deployer or other third - party should be considered to be a provider of a high - risk ai system and therefore assume all the relevant obligations. this would be the case if that party puts its name or trademark on a high - risk ai system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are allocated otherwise. this would also be the case if that party makes a substantial modification to a high - risk ai system that has already been placed on the market or has already been put into service in a way that it remains a high - risk ai system in accordance with this regulation, or if it modifies the intended purpose of an ai system, including a general - purpose ai system, which has not been classified as high - risk and has already been placed on the market or put into service, in a way that the ai system becomes a high - risk ai system in accordance with this regulation. those provisions should apply without prejudice to more specific provisions established in certain union harmonisation legislation based on the new legislative framework, together with which this ( 38 ) directive ( eu ) 2016 / 2102 of the european parliament and of the council of 26 october 2016 on the accessibility of the websites and mobile applications of public sector bodies ( oj l 327'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='57122f7e-4ec7-5c04-a40d-2f21965d8e0e', version=4, score=0.7817549, payload={'response': '1. AI assessing migration and health risks.  2. AI analyzing asylum, visa, and residence permit applications.  3. AI recognizing individuals (excluding travel document verification). 4. AI-powered polygraphs used in migration decisions.', 'meta_data': 'of the council ( 32 ), the directive 2013 / 32 / eu of the european parliament and of the council ( 33 ), and other relevant union law. the use of ai systems in migration, asylum and border control management should, in no circumstances, be used by member states or union institutions, bodies, offices or agencies as a means to circumvent their international obligations under the un convention relating to the status of refugees done at geneva on 28 july 1951 as amended by the protocol of 31 january 1967. nor should they be used to in any way infringe on the principle of non - refoulement, or to deny safe and effective legal avenues into the territory of the union, including the right to international protection. ( 61 ) certain ai systems intended for the administration of justice and democratic processes should be classified as high - risk, considering their potentially significant impact on democracy, the rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. in particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high - risk ai systems intended to be used by a judicial authority or on its behalf to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. ai systems intended to be used by alternative dispute resolution bodies for those purposes should also be considered to be high - risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for the parties. the use of ai tools can support the decision - making power of judges or judicial independence, but should not replace it : the final decision - making must remain a human - driven activity. the classification of ai systems as high - risk should not, however, extend to ai systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks. ( 62 ) without prejudice to the rules provided for in regulation ( eu ) 2024 / 900 of the european parliament and of the council ( 34 ), and in order to address the risks of undue external interference with the right to vote enshrined in article 39 of the charter, and of adverse effects on democracy and the rule of law, ai systems intended to be used to influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda should be'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='17891434-6d44-57db-8f86-0c879354ff2c', version=4, score=0.7772572, payload={'response': '1. 6 months for prohibited AI systems.  2. 12 months for GPAI compliance.  3. 24 months for Annex III high-risk AI systems.  4. 36 months for Annex I high-risk AI systems.Codes of practice must be finalized within 9 months.', 'meta_data': 'regulation by end of 2030 and by 2 august 2030. ( 178 ) providers of high - risk ai systems are encouraged to start to comply, on a voluntary basis, with the relevant obligations of this regulation already during the transitional period. ( 179 ) this regulation should apply from 2 august 2026. however, taking into account the unacceptable risk associated with the use of ai in certain ways, the prohibitions as well as the general provisions of this regulation should already apply from 2 february 2025. while the full effect of those prohibitions follows with the establishment of the governance and enforcement of this regulation, anticipating the application of the prohibitions is important to take account of unacceptable risks and to have an effect on other procedures, such as in civil law. moreover, the infrastructure related to the governance and the conformity assessment system should be operational before 2 august 2026, therefore the provisions on notified bodies and governance structure should apply from 2 august 2025. given the rapid pace of technological advancements and adoption of general - purpose ai models, obligations for providers of general - purpose ai models should apply from 2 august 2025. codes of practice should be ready by 2 may 2025 in view of enabling providers to demonstrate compliance on time. the ai office should ensure that classification rules and procedures are up to date in light of technological developments. in addition, member states should lay down and notify to the commission the rules on penalties, including administrative fines, and ensure that they are properly and effectively implemented by the date of application of this regulation. therefore the provisions on penalties should apply from 2 august 2025. ( 180 ) the european data protection supervisor and the european data protection board were consulted in accordance with article 42 ( 1 ) and ( 2 ) of regulation ( eu ) 2018 / 1725 and delivered their joint opinion on 18 june 2021, have adopted this regulation : chapter i general provisions article 1 subject matter ` 1. the purpose of this regulation is to improve the functioning of the internal market and promote the uptake of human - centric and trustworthy artificial intelligence ( ai ), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the charter, including democracy, the rule of law and environmental protection, against the harmful effects of ai systems in the union and supporting innovation. 2. this regulation lays down : ( a ) harmonised rules for the placing on the market, the putting into service, and the use of ai systems in the union ;'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='86b28d94-a83c-52b9-b820-21ad14820127', version=4, score=0.7672869, payload={'response': '1. AI assessing an individual’s risk of becoming a crime victim.  2. AI evaluating evidence reliability.3. AI profiling individuals during investigations. 4. AI assessing likelihood of offending or re-offending (not solely based on profiling).  5. AI-powered polygraphs used in interrogations.', 'meta_data': 'single out people in a discriminatory or otherwise incorrect or unjust manner. furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such ai systems are not sufficiently transparent, explainable and documented. it is therefore appropriate to classify as high - risk, insofar as their use is permitted under relevant union and national law, a number of ai systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. in view of the nature of the activities and the risks relating thereto, those high - risk ai systems should include in particular ai systems intended to be used by or on behalf of law enforcement authorities or by union institutions, bodies, offices, or agencies in support of law enforcement authorities for assessing the risk of a natural person to become a victim of criminal offences, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course of investigation or prosecution of criminal offences, and, insofar as not prohibited under this regulation, for assessing the risk of a natural person offending or reoffending not solely on the basis of the profiling of natural persons or the assessment of personality traits and characteristics or the past criminal behaviour of natural persons or groups, for profiling in the course of detection, investigation or prosecution of criminal offences. ai systems specifically intended to be used for administrative proceedings by tax and customs authorities as well as by financial intelligence units carrying out administrative tasks analysing information pursuant to union anti - money laundering law should not be classified as high - risk ai systems used by law enforcement authorities for the purpose of prevention, detection, investigation and prosecution of criminal offences. the use of ai tools by law enforcement and other relevant authorities should not become a factor of inequality, or exclusion. the impact of the use of ai tools on the defence rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information on the functioning of those systems and the resulting difficulty in challenging their results in court, in particular by natural persons under investigation. ( 60 ) ai systems used in migration, asylum and border control management affect persons who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. the accuracy, non - discriminatory nature and transparency of the ai systems used in those'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='3479b776-1ea3-512b-a0ae-c48bb6dfcc74', version=4, score=0.7670915, payload={'response': 'Unacceptable risk—banned AI systems (e.g., social scoring, manipulative AI), High risk—strictly regulated AI systems (e.g., medical AI, recruitment AI), Limited risk—transparency obligations (e.g., chatbots, deepfakes), Minimal risk—unregulated AI systems (e.g., spam filters, video games).', 'meta_data': 'classified as high - risk ai systems with the exception of ai systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistical point of view. ( 63 ) the fact that an ai system is classified as a high - risk ai system under this regulation should not be interpreted as indicating that the use of the system is lawful under other acts of union law or under national law compatible with union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. any such use should continue to occur solely in accordance with the applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law. this regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this regulation. ( 64 ) to mitigate the risks from high - risk ai systems placed on the market or put into service and to ensure a high level of trustworthiness, certain mandatory requirements should apply to high - risk ai systems, taking into account the intended purpose and the context of use of the ai system and according to the risk - management system to be established by the provider. the measures adopted by the providers to comply with the mandatory requirements of this regulation should take into account the generally acknowledged state of the art on ai, be proportionate and effective to meet the objectives of this regulation. based on the new legislative framework, as clarified in commission notice ‘ the “ blue guide ” on the implementation of eu product rules 2022 ’, the general rule is that more than one legal act of union harmonisation legislation may be applicable to one product, since the making available or putting into service can take place only when the product complies with all applicable union harmonisation legislation. the hazards of ai systems covered by the requirements of this regulation concern different aspects than the existing union harmonisation legislation and therefore the requirements of this regulation would complement the existing body of the union harmonisation legislation. for example, machinery or medical devices products incorporating an ai system might present risks not addressed by the essential health and safety ( 32 ) regulation ( ec ) no 810 / 2009 of the european parliament and of the council of 13 july 2009 establishing a community code on visas ( visa code ) ( oj l 243, 15. 9. 2009, p. 1 ). ( 33 ) directive 2013 / 32'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='a4298e62-634c-5dba-89ab-a05900355166', version=4, score=0.7668414, payload={'response': 'Deployers have fewer obligations than providers but must ensure compliance when deploying high-risk AI systems in a professional capacity. This applies to deployers in the EU and third-country users whose AI system outputs are used in the EU.', 'meta_data': '2. deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support. 3. the obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under union or national law and to the deployer ’ s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. 4. without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high - risk ai system. 5. deployers shall monitor the operation of the high - risk ai system on the basis of the instructions for use and, where relevant, inform providers in accordance with article 72. where deployers have reason to consider that the use of the high - risk ai system in accordance with the instructions may result in that ai system presenting a risk within the meaning of article 79 ( 1 ), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system. where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. if the deployer is not able to reach the provider, article 73 shall apply mutatis mutandis. this obligation shall not cover sensitive operational data of deployers of ai systems which are law enforcement authorities. for deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under union financial services law, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to the relevant financial service law. 6. deployers of high - risk ai systems shall keep the logs automatically generated by that high - risk ai system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high - risk ai system, of at least six months, unless provided otherwise in applicable union or national law, in particular in union law on the protection of personal data. deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under union financial services law shall maintain the logs as part of the documentation kept pursuant to the relevant union financial service law. 7. before putting into service or using a high - risk ai'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='0b0f8d50-9def-5efc-a0bb-d3870619ce1a', version=4, score=0.7636027, payload={'response': 'The AI Office can evaluate GPAI models to: 1. Assess compliance when information is insufficient.  2. Investigate systemic risks, especially after reports from independent experts.', 'meta_data': '( d ) and ( e ) thereof, the commission is empowered to adopt delegated acts in accordance with article 97 to detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation. 6. the commission is empowered to adopt delegated acts in accordance with article 97 ( 2 ) to amend annexes xi and xii in light of evolving technological developments. 7. any information or documentation obtained pursuant to this article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in article 78. article 54 authorised representatives of providers of general - purpose ai models 1. prior to placing a general - purpose ai model on the union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the union. 2. the provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider. 3. the authorised representative shall perform the tasks specified in the mandate received from the provider. it shall provide a copy of the mandate to the ai office upon request, in one of the official languages of the institutions of the union. for the purposes of this regulation, the mandate shall empower the authorised representative to carry out the following tasks : ( a ) verify that the technical documentation specified in annex xi has been drawn up and all obligations referred to in article 53 and, where applicable, article 55 have been fulfilled by the provider ; ( b ) keep a copy of the technical documentation specified in annex xi at the disposal of the ai office and national competent authorities, for a period of 10 years after the general - purpose ai model has been placed on the market, and the contact details of the provider that appointed the authorised representative ; ( c ) provide the ai office, upon a reasoned request, with all the information and documentation, including that referred to in point ( b ), necessary to demonstrate compliance with the obligations in this chapter ; ( d ) cooperate with the ai office and competent authorities, upon a reasoned request, in any action they take in relation to the general - purpose ai model, including when the model is integrated into ai systems placed on the market or put into service in the union. 4. the mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the ai office or the competent authorities, on all issues related to ensuring compliance with this regulation.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='856f0979-c546-5c0c-b929-a3ce0d57c3f5', version=4, score=0.7604859, payload={'response': '1. Biometric categorization systems inferring sensitive attributes.  2. AI systems managing critical infrastructure like water and electricity supply.3. Systems determining access to education or vocational training.4. Recruitment and worker management systems.5. AI systems assessing eligibility for public services or creditworthiness.  6. Law enforcement systems evaluating evidence reliability or profiling individuals.  7. Migration and border control systems assessing asylum applications. 8. Systems influencing elections or voting behavior.', 'meta_data': 'annex iii high - risk ai systems referred to in article 6 ( 2 ) high - risk ai systems pursuant to article 6 ( 2 ) are the ai systems listed in any of the following areas : 1. biometrics, in so far as their use is permitted under relevant union or national law : ( a ) remote biometric identification systems. this shall not include ai systems intended to be used for biometric verification the sole purpose of which is to confirm that a specific natural person is the person he or she claims to be ; ( b ) ai systems intended to be used for biometric categorisation, according to sensitive or protected attributes or characteristics based on the inference of those attributes or characteristics ; ( c ) ai systems intended to be used for emotion recognition. 2. critical infrastructure : ai systems intended to be used as safety components in the management and operation of critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity. 3. education and vocational training : ( a ) ai systems intended to be used to determine access or admission or to assign natural persons to educational and vocational training institutions at all levels ; ( b ) ai systems intended to be used to evaluate learning outcomes, including when those outcomes are used to steer the learning process of natural persons in educational and vocational training institutions at all levels ; ( c ) ai systems intended to be used for the purpose of assessing the appropriate level of education that an individual will receive or will be able to access, in the context of or within educational and vocational training institutions at all levels ; ( d ) ai systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in the context of or within educational and vocational training institutions at all levels. 4. employment, workers ’ management and access to self - employment : ( a ) ai systems intended to be used for the recruitment or selection of natural persons, in particular to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates ; ( b ) ai systems intended to be used to make decisions affecting terms of work - related relationships, the promotion or termination of work - related contractual relationships, to allocate tasks based on individual behaviour or personal traits or characteristics or to monitor and evaluate the performance and behaviour of persons in such relationships. 5. access to and enjoyment of essential private services and essential public services and benefits : ( a ) ai systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for essential public assistance benefits and services, including'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='749a84ca-6d49-5534-921f-f9757d32717d', version=4, score=0.7587357, payload={'response': '1. Implement a risk management system throughout the AI lifecycle.2. Ensure training datasets are representative and error-free.3. Maintain technical documentation demonstrating compliance.4. Enable automatic record-keeping for risk identification.5. Provide instructions for compliance to deployers.6. Design systems with human oversight mechanisms', 'meta_data': 'section 3 obligations of providers and deployers of high - risk ai systems and other parties article 16 obligations of providers of high - risk ai systems providers of high - risk ai systems shall : ( a ) ensure that their high - risk ai systems are compliant with the requirements set out in section 2 ; ( b ) indicate on the high - risk ai system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted ; ( c ) have a quality management system in place which complies with article 17 ; ( d ) keep the documentation referred to in article 18 ; ( e ) when under their control, keep the logs automatically generated by their high - risk ai systems as referred to in article 19 ; ( f ) ensure that the high - risk ai system undergoes the relevant conformity assessment procedure as referred to in article 43, prior to its being placed on the market or put into service ; ( g ) draw up an eu declaration of conformity in accordance with article 47 ; ( h ) affix the ce marking to the high - risk ai system or, where that is not possible, on its packaging or its accompanying documentation, to indicate conformity with this regulation, in accordance with article 48 ; ( i ) comply with the registration obligations referred to in article 49 ( 1 ) ; ( j ) take the necessary corrective actions and provide information as required in article 20 ; ( k ) upon a reasoned request of a national competent authority, demonstrate the conformity of the high - risk ai system with the requirements set out in section 2 ; ( l ) ensure that the high - risk ai system complies with accessibility requirements in accordance with directives ( eu ) 2016 / 2102 and ( eu ) 2019 / 882. article 17 quality management system 1. providers of high - risk ai systems shall put a quality management system in place that ensures compliance with this regulation. that system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects : ( a ) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high - risk ai system ; ( b ) techniques, procedures and systematic actions to be used for the design, design control and design verification of the high - risk ai system ; ( c ) techniques, procedures and systematic actions to be used for the development, quality control and'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='a17f3e8f-d21b-5aaf-b398-588c51bb07df', version=4, score=0.74525964, payload={'response': 'A GPAI model is systemic if its training compute exceeds \\\\\\\\( 10^{25} \\\\\\\\) FLOPS. Providers must notify the European Commission within two weeks of exceeding this threshold.', 'meta_data': 'of general - purpose ai models as general - purpose ai model with systemic risks. since systemic risks result from particularly high capabilities, a general - purpose ai model should be considered to present systemic risks if it has high - impact capabilities, evaluated on the basis of appropriate technical tools and methodologies, or significant impact on the internal market due to its reach. high - impact capabilities in general - purpose ai models means capabilities that match or exceed the capabilities recorded in the most advanced general - purpose ai models. the full range of capabilities in a model could be better understood after its placing on the market or when deployers interact with the model. according to the state of the art at the time of entry into force of this regulation, the cumulative amount of computation used for the training of the general - purpose ai model measured in floating point operations is one of the relevant approximations for model capabilities. the cumulative amount of computation used for training includes the computation used across the activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre - training, synthetic data generation and fine - tuning. therefore, an initial threshold of floating point operations should be set, which, if met by a general - purpose ai model, leads to a presumption that the model is a general - purpose ai model with systemic risks. this threshold should be adjusted over time to reflect technological and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be supplemented with benchmarks and indicators for model capability. to inform this, the ai office should engage with the scientific community, industry, civil society and other experts. thresholds, as well as tools and benchmarks for the assessment of high - impact capabilities, should be strong predictors of generality, its capabilities and associated systemic risk of general - purpose ai models, and could take into account the way the model will be placed on the market or the number of users it may affect. to complement this system, there should be a possibility for the commission to take individual decisions designating a general - purpose ai model as a general - purpose ai model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold. that decision should be taken on the basis of an overall assessment of the criteria for the designation of a general - purpose ai model with systemic risk set out in an annex to this regulation, such as quality or size of the training data set, number of business and end users, its input and output modalities, its level of'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6d19d079-a999-52da-b066-69c77d098627', version=4, score=0.7416184, payload={'response': '1. AI systems using manipulative techniques to distort behavior and impair decision-making.  2. Social scoring systems evaluating individuals based on personal traits.3. Systems assessing criminal risk solely based on profiling.4. Untargeted facial recognition databases.5. Emotion inference in workplaces or schools (except for medical or safety reasons).  6. Real-time remote biometric identification in public spaces (with exceptions for law enforcement).', 'meta_data': 'article 4 ai literacy providers and deployers of ai systems shall take measures to ensure, to their best extent, a sufficient level of ai literacy of their staff and other persons dealing with the operation and use of ai systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the ai systems are to be used in, and considering the persons or groups of persons on whom the ai systems are to be used. chapter ii prohibited ai practices article 5 prohibited ai practices 1. the following ai practices shall be prohibited : ( a ) the placing on the market, the putting into service or the use of an ai system that deploys subliminal techniques beyond a person ’ s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm ; ( b ) the placing on the market, the putting into service or the use of an ai system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm ; ( c ) the placing on the market, the putting into service or the use of ai systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following : ( i ) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected ; ( ii ) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity ; ( d ) the placing on the market, the putting into service for this specific purpose, or the use of an ai system for making risk assessments of natural persons in order to assess or predict the risk of a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='c8581c61-1349-5c54-b587-382b73dcc9da', version=4, score=0.737363, payload={'response': 'High-risk AI systems must be designed to allow deployers to implement human oversight, ensuring that decisions can be reviewed and overridden if necessary.', 'meta_data': '12. article 14 human oversight 1. high - risk ai systems shall be designed and developed in such a way, including with appropriate human - machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use. 2. human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high - risk ai system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this section. 3. the oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high - risk ai system, and shall be ensured through either one or both of the following types of measures : ( a ) measures identified and built, when technically feasible, into the high - risk ai system by the provider before it is placed on the market or put into service ; ( b ) measures identified by the provider before placing the high - risk ai system on the market or putting it into service and that are appropriate to be implemented by the deployer. 4. for the purpose of implementing paragraphs 1, 2 and 3, the high - risk ai system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate : ( a ) to properly understand the relevant capacities and limitations of the high - risk ai system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance ; ( b ) to remain aware of the possible tendency of automatically relying or over - relying on the output produced by a high - risk ai system ( automation bias ), in particular for high - risk ai systems used to provide information or recommendations for decisions to be taken by natural persons ; ( c ) to correctly interpret the high - risk ai system ’ s output, taking into account, for example, the interpretation tools and methods available ;'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='b029fb58-c29b-5c71-863e-c36bbdaa9c40', version=4, score=0.7353422, payload={'response': 'AI systems that interact with voters to influence voting behavior are high-risk, while AI tools that optimize campaign logistics without direct voter engagement are not classified as high-risk.', 'meta_data': 'applying the law to a concrete set of facts. ai systems intended to be used by alternative dispute resolution bodies for those purposes should also be considered to be high - risk when the outcomes of the alternative dispute resolution proceedings produce legal effects for the parties. the use of ai tools can support the decision - making power of judges or judicial independence, but should not replace it : the final decision - making must remain a human - driven activity. the classification of ai systems as high - risk should not, however, extend to ai systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks. ( 62 ) without prejudice to the rules provided for in regulation ( eu ) 2024 / 900 of the european parliament and of the council ( 34 ), and in order to address the risks of undue external interference with the right to vote enshrined in article 39 of the charter, and of adverse effects on democracy and the rule of law, ai systems intended to be used to influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda should be classified as high - risk ai systems with the exception of ai systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistical point of view. ( 63 ) the fact that an ai system is classified as a high - risk ai system under this regulation should not be interpreted as indicating that the use of the system is lawful under other acts of union law or under national law compatible with union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. any such use should continue to occur solely in accordance with the applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law. this regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant, unless it is specifically otherwise provided for in this regulation. ( 64 ) to mitigate the risks from high - risk ai systems placed on the market or put into service and to ensure a high level of trustworthiness, certain mandatory requirements should apply to high - risk ai systems, taking into account the intended purpose and the context of use of the ai system'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='409a2f2f-6cee-53bd-8882-4155f9c92ad0', version=4, score=0.7340894, payload={'response': 'Providers must establish a risk management system throughout the AI system’s lifecycle, addressing risks related to health, safety, and fundamental rights.', 'meta_data': '2. where a product contains an ai system, to which the requirements of this regulation as well as requirements of the union harmonisation legislation listed in section a of annex i apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable union harmonisation legislation. in ensuring the compliance of high - risk ai systems referred to in paragraph 1 with the requirements set out in this section, and in order to ensure consistency, avoid duplication and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into documentation and procedures that already exist and are required under the union harmonisation legislation listed in section a of annex i. article 9 risk management system 1. a risk management system shall be established, implemented, documented and maintained in relation to high - risk ai systems. 2. the risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high - risk ai system, requiring regular systematic review and updating. it shall comprise the following steps : ( a ) the identification and analysis of the known and the reasonably foreseeable risks that the high - risk ai system can pose to health, safety or fundamental rights when the high - risk ai system is used in accordance with its intended purpose ; ( b ) the estimation and evaluation of the risks that may emerge when the high - risk ai system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse ; ( c ) the evaluation of other risks possibly arising, based on the analysis of data gathered from the post - market monitoring system referred to in article 72 ; ( d ) the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point ( a ). 3. the risks referred to in this article shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high - risk ai system, or the provision of adequate technical information. 4. the risk management measures referred to in paragraph 2, point ( d ), shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this section, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements. 5. the risk management measures referred to in paragraph 2, point ( d ), shall be such that the relevant residual risk associated with each hazard,'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='30c8b5a3-4863-5906-aa42-8f0fc9056d7d', version=4, score=0.7333877, payload={'response': 'Developers and deployers must ensure that end-users are aware they are interacting with AI, such as chatbots and deepfakes.', 'meta_data': 'specific ai system for the purposes of this regulation. that initial provider shall closely cooperate with new providers and shall make available the necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the obligations set out in this regulation, in particular regarding the compliance with the conformity assessment of high - risk ai systems. this paragraph shall not apply in cases where the initial provider has clearly specified that its ai system is not to be changed into a high - risk ai system and therefore does not fall under the obligation to hand over the documentation. 3. in the case of high - risk ai systems that are safety components of products covered by the union harmonisation legislation listed in section a of annex i, the product manufacturer shall be considered to be the provider of the high - risk ai system, and shall be subject to the obligations under article 16 under either of the following circumstances : ( a ) the high - risk ai system is placed on the market together with the product under the name or trademark of the product manufacturer ; ( b ) the high - risk ai system is put into service under the name or trademark of the product manufacturer after the product has been placed on the market. 4. the provider of a high - risk ai system and the third party that supplies an ai system, tools, services, components, or processes that are used or integrated in a high - risk ai system shall, by written agreement, specify the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high - risk ai system to fully comply with the obligations set out in this regulation. this paragraph shall not apply to third parties making accessible to the public tools, services, processes, or components, other than general - purpose ai models, under a free and open - source licence. the ai office may develop and recommend voluntary model terms for contracts between providers of high - risk ai systems and third parties that supply tools, services, components or processes that are used for or integrated into high - risk ai systems. when developing those voluntary model terms, the ai office shall take into account possible contractual requirements applicable in specific sectors or business cases. the voluntary model terms shall be published and be available free of charge in an easily usable electronic format. 5. paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets in accordance with union and national law. article 26 obligations of deployers of high - risk ai systems'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='88dc63e0-fd0a-580c-99fa-5ec0ae9cd0cc', version=4, score=0.72543985, payload={'response': '1. AI filtering and analyzing job applications.2. AI evaluating candidate suitability. 3. AI allocating tasks based on performance metrics.  4. AI making promotion and termination decisions.', 'meta_data': 'regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of ai systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers. 12. this regulation does not apply to ai systems released under free and open - source licences, unless they are placed on the market or put into service as high - risk ai systems or as an ai system that falls under article 5 or 50. article 3 definitions for the purposes of this regulation, the following definitions apply : ( 1 ) ‘ ai system ’ means a machine - based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments ; ( 2 ) ‘ risk ’ means the combination of the probability of an occurrence of harm and the severity of that harm ; ( 3 ) ‘ provider ’ means a natural or legal person, public authority, agency or other body that develops an ai system or a general - purpose ai model or that has an ai system or a general - purpose ai model developed and places it on the market or puts the ai system into service under its own name or trademark, whether for payment or free of charge ; ( 4 ) ‘ deployer ’ means a natural or legal person, public authority, agency or other body using an ai system under its authority except where the ai system is used in the course of a personal non - professional activity ; ( 5 ) ‘ authorised representative ’ means a natural or legal person located or established in the union who has received and accepted a written mandate from a provider of an ai system or a general - purpose ai model to, respectively, perform and carry out on its behalf the obligations and procedures established by this regulation ; ( 6 ) ‘ importer ’ means a natural or legal person located or established in the union that places on the market an ai system that bears the name or trademark of a natural or legal person established in a third country ; ( 7 ) ‘ distributor ’ means a natural or legal person in the supply chain, other than the provider or the importer, that makes an ai system available on the union market ; ( 8 ) ‘ operator ’ means a provider, product manufacturer, deployer, authorised representative, importer or distributor ;'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg.retrieve(\"What are the key points of the European AI Act 2024?\", 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
